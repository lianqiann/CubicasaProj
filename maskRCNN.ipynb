{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from floortrans.loaders import FloorplanSVG, DictToTensor, Compose, RotateNTurns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "class CubicasaDataset(object):\n",
    "    def __init__(self, root, mode, transforms=None):\n",
    "        self.root = root\n",
    "        self.dict = torch.load(f\"data/cubicasa5k/instance_info_{mode}.pt\")\n",
    "        self.transforms = transforms\n",
    "        self.imgs = np.genfromtxt(root + '/'+mode+'.txt', dtype='str')\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        \n",
    "        instance_info = self.dict[self.imgs[idx]]\n",
    "        \n",
    "#         fplan = cv2.imread()\n",
    "#         img = cv2.cvtColor(fplan, cv2.COLOR_BGR2RGB)  # correct color channels\n",
    "        img = Image.open(self.root + self.imgs[idx]+'F1_original.png').convert(\"RGB\")\n",
    "        #img = np.moveaxis(fplan, -1, 0)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = instance_info['boxes']\n",
    "        target[\"labels\"] = instance_info['labels'].long()\n",
    "        target[\"masks\"] = torch.as_tensor(instance_info['masks'], dtype=torch.uint8)\n",
    "        target[\"image_id\"] = torch.tensor([idx], dtype = torch.int8)\n",
    "        target[\"area\"] = instance_info['area']\n",
    "        target[\"iscrowd\"] = instance_info['iscrowd']\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from floortrans.loaders.house import House\n",
    "import copy\n",
    "\n",
    "room_classes = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\", \"Undefined\"]\n",
    "rooms = [\"Outdoor\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\", \"Undefined\"]\n",
    "room_ids = [1,3,4,5,6,7,8,9,10,11]\n",
    "room_labels = {rooms[i]:i+1 for i in range(len(rooms))}\n",
    "\n",
    "\n",
    "class CubicasaDataset(object):\n",
    "    def __init__(self, root, mode, transforms=None):\n",
    "        self.root = root\n",
    "        #self.dict = torch.load(f\"data/cubicasa5k/instance_info_{mode}.pt\")\n",
    "        self.transforms = transforms\n",
    "        self.imgs = np.genfromtxt(root + '/'+mode+'.txt', dtype='str')\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        \n",
    "        #instance_info = self.dict[self.imgs[idx]]\n",
    "        \n",
    "        # fplan = cv2.imread(self.root + self.imgs[idx]+'F1_original.png')\n",
    "        # img = cv2.cvtColor(fplan, cv2.COLOR_BGR2RGB)/255.  # correct color channels\n",
    "\n",
    "        org_img_path = self.root + self.imgs[idx]+'F1_original.png'\n",
    "        img_path = self.root + self.imgs[idx]+'F1_scaled.png'\n",
    "        svg_path = self.root + self.imgs[idx]+'model.svg'\n",
    "\n",
    "        img = Image.open(org_img_path).convert(\"RGB\")\n",
    "\n",
    "        height, width, _ = cv2.imread(img_path).shape\n",
    "        #height_org, width_org, _ = cv2.imread(org_img_path).shape\n",
    "\n",
    "        # Getting labels for segmentation and heatmaps\n",
    "        house = House(svg_path, height, width)\n",
    "        # Combining them to one numpy tensor\n",
    "        label = torch.tensor(house.get_segmentation_tensor().astype(np.float32))\n",
    "\n",
    "        label = label.unsqueeze(0)\n",
    "        label = torch.nn.functional.interpolate(label,\n",
    "                                                    size=(height, width),\n",
    "                                                    mode='nearest')\n",
    "        label = label.squeeze(0)[0]\n",
    "\n",
    "\n",
    "        #############process items##############\n",
    "        masks = label.data.numpy()\n",
    "    \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        num_obj = 0\n",
    "        \n",
    "        mask_tensor = []\n",
    "        areas = []\n",
    "        \n",
    "        for r in room_ids:\n",
    "            x = copy.copy(masks)\n",
    "            x[masks != r] = 0 \n",
    "            x = x.astype(np.uint8)\n",
    "            contours, _ = cv2.findContours(x,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            for tcnt in contours: \n",
    "                \n",
    "                im = np.zeros((height,width,3), np.uint8)\n",
    "                im = cv2.drawContours(im, [tcnt], -1, (255,255,255), -1)\n",
    "                mask_tensor.append((im[:,:,0]/255).astype(np.int8))\n",
    "                areas.append(cv2.contourArea(tcnt,False) )\n",
    "                x,y,w,h = cv2.boundingRect(tcnt)\n",
    "                boxes.append([x,y,x+w,y+h])\n",
    "                labels.append(room_labels[room_classes[r]])\n",
    "                num_obj+=1\n",
    "        \n",
    "        try:\n",
    "            mask_tensor = np.stack(mask_tensor, 0)\n",
    "        except:\n",
    "            mask_tensor = np.array([])\n",
    "\n",
    "\n",
    "        #######################################\n",
    "\n",
    "        #img = np.moveaxis(fplan, -1, 0)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes)\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype = torch.long)\n",
    "        target[\"masks\"] = torch.as_tensor(mask_tensor, dtype=torch.uint8)\n",
    "        target[\"image_id\"] = torch.tensor([idx], dtype = torch.int8)\n",
    "        target[\"area\"] = torch.as_tensor(areas)\n",
    "        target[\"iscrowd\"] = torch.zeros(num_obj, dtype = torch.int8)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 10+1  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=9,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "import utils\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "dataset = CubicasaDataset('data/cubicasa5k', 'test',get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    " dataset, batch_size=2, shuffle=True,collate_fn=utils.collate_fn)\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images,targets)   # Returns losses and detections\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)           # Returns predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "\n",
    "def main():\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = 1+10\n",
    "    # use our dataset and defined transformations\n",
    "    dataset = CubicasaDataset('data/cubicasa5k', 'val',get_transform(train=True))\n",
    "\n",
    "    dataset_test = CubicasaDataset('data/cubicasa5k', 'test',get_transform(train=False))\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=True, \n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, \n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    # get the model using our helper function\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "    # let's train it for 10 epochs\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 1+10\n",
    "# use our dataset and defined transformations\n",
    "dataset = CubicasaDataset('data/cubicasa5k', 'val',get_transform(train=True))\n",
    "\n",
    "dataset_test = CubicasaDataset('data/cubicasa5k', 'test',get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=True, \n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, \n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device == torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-87c7183c6c05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DS2-final/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup_lr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DS2-final/utils.py\u001b[0m in \u001b[0;36mlog_every\u001b[0;34m(self, iterable, print_freq, device, header)\u001b[0m\n\u001b[1;32m    211\u001b[0m                         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                         \u001b[0mmeters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                         time=str(iter_time), data=str(data_time), memory = 0))\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'memory'"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e3378780>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.moveaxis(images[0].numpy().data,0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
