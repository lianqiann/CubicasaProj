{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from floortrans.loaders import FloorplanSVG, DictToTensor, Compose, RotateNTurns\n",
    "from floortrans.loaders.house import House\n",
    "import copy\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import transforms as T\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "class CubicasaDataset(object):\n",
    "    def __init__(self, root, mode, transforms=None):\n",
    "        self.root = root\n",
    "        #self.dict = torch.load(f\"data/cubicasa5k/instance_info_{mode}.pt\")\n",
    "        self.transforms = transforms\n",
    "        self.imgs = np.genfromtxt(root + '/'+mode+'.txt', dtype='str')\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        \n",
    "        #instance_info = self.dict[self.imgs[idx]]\n",
    "        \n",
    "        # fplan = cv2.imread(self.root + self.imgs[idx]+'F1_original.png')\n",
    "        # img = cv2.cvtColor(fplan, cv2.COLOR_BGR2RGB)/255.  # correct color channels\n",
    "\n",
    "        org_img_path = self.root + self.imgs[idx]+'F1_original.png'\n",
    "        img_path = self.root + self.imgs[idx]+'F1_scaled.png'\n",
    "        svg_path = self.root + self.imgs[idx]+'model.svg'\n",
    "\n",
    "        img = Image.open(org_img_path).convert(\"RGB\")\n",
    "\n",
    "        height, width, _ = cv2.imread(img_path).shape\n",
    "        height_org, width_org, _ = cv2.imread(org_img_path).shape\n",
    "\n",
    "        # Getting labels for segmentation and heatmaps\n",
    "        house = House(svg_path, height, width)\n",
    "        # Combining them to one numpy tensor\n",
    "        label = torch.tensor(house.get_segmentation_tensor().astype(np.float32))\n",
    "\n",
    "        label = label.unsqueeze(0)\n",
    "        label = torch.nn.functional.interpolate(label,\n",
    "                                                    size=(height_org, width_org),\n",
    "                                                    mode='nearest')\n",
    "        label = label.squeeze(0)[0]\n",
    "\n",
    "\n",
    "        #############process items##############\n",
    "        masks = label.data.numpy()\n",
    "    \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        num_obj = 0\n",
    "        \n",
    "        mask_tensor = []\n",
    "        areas = []\n",
    "\n",
    "        limit_list = []\n",
    "\n",
    "        for r in room_ids:\n",
    "            x = copy.copy(masks)\n",
    "            x[masks != r] = 0 \n",
    "            x = x.astype(np.uint8)\n",
    "            contours, _ = cv2.findContours(x,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n",
    "            limit_list +=[(r, cot) for cot in contours]\n",
    "            num_obj+=len(contours)\n",
    "        \n",
    "        if num_obj >20:\n",
    "            rand_inds = np.random.choice(np.arange(num_obj), 20, replace  = False)\n",
    "        elif num_obj == 0:\n",
    "            rand_inds = []\n",
    "            print('No objects in this image, folder:', self.imgs[idx])\n",
    "        else:\n",
    "            rand_inds = np.arange(num_obj)\n",
    "        \n",
    "        for ind in rand_inds:\n",
    "            r, tcnt = limit_list[ind]\n",
    "            im = np.zeros((height,width,3), np.uint8)\n",
    "            im = cv2.drawContours(im, [tcnt], -1, (255,255,255), -1)\n",
    "            mask_tensor.append((im[:,:,0]/255).astype(np.int8))\n",
    "            areas.append(cv2.contourArea(tcnt,False))\n",
    "            x,y,w,h = cv2.boundingRect(tcnt)\n",
    "            boxes.append([x,y,x+w,y+h])\n",
    "            labels.append(room_labels[room_classes[r]])\n",
    "        \n",
    "        boxes = torch.FloatTensor(boxes)\n",
    "        labels = torch.as_tensor(labels, dtype = torch.long)\n",
    "        areas = torch.FloatTensor(areas)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            mask_tensor = np.stack(mask_tensor, 0)\n",
    "        except:\n",
    "            mask_tensor = np.array([])\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = torch.as_tensor(mask_tensor, dtype=torch.uint8)\n",
    "        target[\"image_id\"] = torch.tensor([idx], dtype = torch.int8)\n",
    "        \n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = torch.zeros(num_obj, dtype = torch.int8)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 1+10\n",
    "# use our dataset and defined transformations\n",
    "dataset = CubicasaDataset('data/cubicasa5k', 'train',get_transform(train=True))\n",
    "\n",
    "dataset_test = CubicasaDataset('data/cubicasa5k', 'val',get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=False, \n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, \n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.9961, 0.9961, 0.9961],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.8863, 0.8863, 0.8863,  ..., 0.8863, 0.8863, 0.8863]],\n",
       " \n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 0.9961, 0.9961, 0.9961],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.8941, 0.8941, 0.8941,  ..., 0.8941, 0.8941, 0.8941]],\n",
       " \n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 0.9961, 0.9961, 0.9961],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9059, 0.9059, 0.9059,  ..., 0.9059, 0.9059, 0.9059]]]),\n",
       " {'boxes': tensor([[225., 348., 277., 381.],\n",
       "          [258., 225., 427., 325.],\n",
       "          [281., 325., 369., 377.],\n",
       "          [225.,  15., 327., 110.],\n",
       "          [331.,  15., 427., 110.],\n",
       "          [225., 252., 254., 293.],\n",
       "          [225., 297., 277., 344.],\n",
       "          [220., 348., 225., 381.],\n",
       "          [225., 162., 271., 248.],\n",
       "          [225., 114., 271., 157.],\n",
       "          [275., 114., 427., 225.]]),\n",
       "  'labels': tensor([ 1,  2,  3,  4,  4,  5,  6,  7, 10, 10, 10]),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       "  'image_id': tensor([-3], dtype=torch.int8),\n",
       "  'area': tensor([ 1632.0000, 13787.0000,  4437.0000,  9494.0000,  8930.0000,  1120.0000,\n",
       "           2053.5000,   128.0000,  3676.5000,  1890.0000, 16608.0000]),\n",
       "  'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int8)})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
