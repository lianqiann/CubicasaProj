{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from house import House\n",
    "import copy\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import transforms as T\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_classes = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\", \"Undefined\"]\n",
    "rooms = [\"Outdoor\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\", \"Undefined\"]\n",
    "room_ids = [1,3,4,5,6,7,8,9,10,11]\n",
    "room_labels = {rooms[i]:i+1 for i in range(len(rooms))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "class CubicasaDataset(object):\n",
    "    def __init__(self, root, mode, transforms=None):\n",
    "        self.root = root\n",
    "        #self.dict = torch.load(f\"data/cubicasa5k/instance_info_{mode}.pt\")\n",
    "        self.transforms = transforms\n",
    "        self.imgs = np.genfromtxt(root + '/'+mode+'.txt', dtype='str')\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        \n",
    "        #instance_info = self.dict[self.imgs[idx]]\n",
    "        \n",
    "        # fplan = cv2.imread(self.root + self.imgs[idx]+'F1_original.png')\n",
    "        # img = cv2.cvtColor(fplan, cv2.COLOR_BGR2RGB)/255.  # correct color channels\n",
    "\n",
    "        org_img_path = self.root + self.imgs[idx]+'F1_original.png'\n",
    "        img_path = self.root + self.imgs[idx]+'F1_scaled.png'\n",
    "        svg_path = self.root + self.imgs[idx]+'model.svg'\n",
    "\n",
    "        img = Image.open(org_img_path).convert(\"RGB\")\n",
    "\n",
    "        height, width, _ = cv2.imread(img_path).shape\n",
    "        height_org, width_org, _ = cv2.imread(org_img_path).shape\n",
    "\n",
    "        # Getting labels for segmentation and heatmaps\n",
    "        house = House(svg_path, height, width)\n",
    "        # Combining them to one numpy tensor\n",
    "        label = torch.tensor(house.get_segmentation_tensor().astype(np.float32))\n",
    "\n",
    "        label = label.unsqueeze(0)\n",
    "        label = torch.nn.functional.interpolate(label,\n",
    "                                                    size=(height_org, width_org),\n",
    "                                                    mode='nearest')\n",
    "        label = label.squeeze(0)[0]\n",
    "\n",
    "\n",
    "        #############process items##############\n",
    "        masks = label.data.numpy()\n",
    "    \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        num_obj = 0\n",
    "        \n",
    "        mask_tensor = []\n",
    "        areas = []\n",
    "\n",
    "        limit_list = []\n",
    "\n",
    "        for r in room_ids:\n",
    "            x = copy.copy(masks)\n",
    "            x[masks != r] = 0 \n",
    "            x = x.astype(np.uint8)\n",
    "            contours, _ = cv2.findContours(x,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n",
    "            limit_list +=[(r, cot) for cot in contours]\n",
    "            num_obj+=len(contours)\n",
    "        \n",
    "        if num_obj >20:\n",
    "            rand_inds = np.random.choice(np.arange(num_obj), 20, replace  = False)\n",
    "        elif num_obj == 0:\n",
    "            rand_inds = []\n",
    "            print('No objects in this image, folder:', self.imgs[idx])\n",
    "        else:\n",
    "            rand_inds = np.arange(num_obj)\n",
    "        \n",
    "        for ind in rand_inds:\n",
    "            r, tcnt = limit_list[ind]\n",
    "            im = np.zeros((height,width,3), np.uint8)\n",
    "            im = cv2.drawContours(im, [tcnt], -1, (255,255,255), -1)\n",
    "            mask_tensor.append((im[:,:,0]/255).astype(np.int8))\n",
    "            areas.append(cv2.contourArea(tcnt,False))\n",
    "            x,y,w,h = cv2.boundingRect(tcnt)\n",
    "            boxes.append([x,y,x+w,y+h])\n",
    "            labels.append(room_labels[room_classes[r]])\n",
    "        \n",
    "        boxes = torch.FloatTensor(boxes)\n",
    "        labels = torch.as_tensor(labels, dtype = torch.long)\n",
    "        areas = torch.FloatTensor(areas)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            mask_tensor = np.stack(mask_tensor, 0)\n",
    "        except:\n",
    "            mask_tensor = np.array([])\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = torch.as_tensor(mask_tensor, dtype=torch.uint8)\n",
    "        target[\"image_id\"] = torch.tensor([idx], dtype = torch.int8)\n",
    "        \n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = torch.zeros(num_obj, dtype = torch.int8)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 1+10\n",
    "# use our dataset and defined transformations\n",
    "dataset = CubicasaDataset('data/cubicasa5k', 'train',get_transform(train=True))\n",
    "\n",
    "dataset_test = CubicasaDataset('data/cubicasa5k', 'val',get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "#indices = torch.randperm(len(dataset)).tolist()\n",
    "#dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "#dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=False, \n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, \n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.genfromtxt('data/cubicasa5k' + '/'+'train'+'.txt', dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/high_quality_architectural/4520/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No objects in this image, folder: /high_quality_architectural/4520/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.1255, 0.2314,  ..., 0.2510, 0.3333, 0.1216],\n",
       "          [0.1255, 0.3725, 0.5098,  ..., 0.5333, 0.4078, 0.1608],\n",
       "          [0.2353, 0.5137, 0.6824,  ..., 0.7020, 0.4706, 0.1843],\n",
       "          ...,\n",
       "          [0.3765, 0.6510, 0.8000,  ..., 0.8314, 0.4314, 0.1490],\n",
       "          [0.2980, 0.5255, 0.6510,  ..., 0.6196, 0.3294, 0.1098],\n",
       "          [0.0000, 0.1098, 0.2039,  ..., 0.1176, 0.2157, 0.0549]],\n",
       " \n",
       "         [[1.0000, 0.9922, 0.8392,  ..., 0.8314, 0.8392, 0.8431],\n",
       "          [0.9922, 1.0000, 0.9020,  ..., 0.8863, 0.9059, 0.8784],\n",
       "          [0.8431, 0.9020, 0.8157,  ..., 0.7922, 0.9569, 0.8941],\n",
       "          ...,\n",
       "          [0.7255, 0.8157, 0.7529,  ..., 0.6980, 0.9961, 0.9059],\n",
       "          [0.8588, 0.9059, 0.8353,  ..., 0.8392, 0.9804, 0.9294],\n",
       "          [0.9490, 0.9451, 0.8549,  ..., 0.9098, 0.9412, 0.9412]],\n",
       " \n",
       "         [[0.0000, 0.0980, 0.2275,  ..., 0.2431, 0.3843, 0.1529],\n",
       "          [0.0980, 0.3725, 0.5255,  ..., 0.5412, 0.4549, 0.1882],\n",
       "          [0.2314, 0.5373, 0.7216,  ..., 0.7294, 0.5098, 0.2078],\n",
       "          ...,\n",
       "          [0.3725, 0.6196, 0.7608,  ..., 0.8353, 0.4118, 0.1647],\n",
       "          [0.3059, 0.5020, 0.6235,  ..., 0.6549, 0.3137, 0.1216],\n",
       "          [0.0000, 0.1059, 0.1882,  ..., 0.1412, 0.2000, 0.0627]]]),\n",
       " {'boxes': tensor([]),\n",
       "  'labels': tensor([], dtype=torch.int64),\n",
       "  'masks': tensor([], dtype=torch.uint8),\n",
       "  'image_id': tensor([-23], dtype=torch.int8),\n",
       "  'area': tensor([]),\n",
       "  'iscrowd': tensor([], dtype=torch.int8)})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " {'boxes': tensor([[ 50., 159., 141., 468.],\n",
       "          [158., 524., 393., 735.],\n",
       "          [497., 745., 582., 852.],\n",
       "          [158., 743., 393., 941.],\n",
       "          [400., 551., 580., 656.],\n",
       "          [766., 532., 979., 737.],\n",
       "          [725., 332., 979., 525.],\n",
       "          [725., 101., 979., 367.],\n",
       "          [157., 101., 802., 941.]]),\n",
       "  'labels': tensor([ 1,  2, 10, 10, 10, 10, 10, 10, 10]),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       "  'image_id': tensor([15], dtype=torch.int8),\n",
       "  'area': tensor([ 27720.0000,  49140.0000,   8904.0000,  46098.0000,  18616.0000,\n",
       "           40036.5000,  40806.5000,  63811.5000, 283733.0000]),\n",
       "  'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int8)})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        if args.val !='None':\n",
    "            try:\n",
    "                evaluate(model, data_loader_val, device=device)\n",
    "            except:\n",
    "                print('evaluation encouters problem!')\n",
    "\n",
    "            torch.save(model.state_dict(), f'checkpoints/{args.model_name}_{epoch}.pt')\n",
    "\n",
    "        print('*'*25+f'epoch {epoch} finished'+'*'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
